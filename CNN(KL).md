卷积运算通过三个重要的思想来帮助改进机器学习系统:稀疏交互 (sparse interactions)、参数共享 (parameter sharing)、等变表示 (equivariant representations)。

传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵的每一个独立的参数都描述了每一个输入单元与每一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而，卷积神经网络具有稀疏交互 (sparse interactions)(也叫做稀疏连接 (sparse connectivity) 或者稀疏权重 (sparse weights))的特征。这通过使得核的规模远小于输入的规模来实现。举 个例子，当进行图像处理时，输入的图像可能包含百万个像素点，但是我们可以通过只占用几十到上百个像素点的核来探测一些小的有意义的特征，例如图像的边缘。 这意味着我们需要存储的参数更少，不仅减少了模型的存储需求，而且提高了它的统计效率。这也意味着为了得到输出我们只需要更少的计算量。这些效率上的提高往往是很显著的。如果有 m 个输入和 n 个输出，那么矩阵乘法需要 m × n 个参数 并且相应算法的时间复杂度为 O(m × n)(对于每一个例子)。如果我们限制每一个 输出拥有的连接数为 k，那么稀疏的连接方法只需要 k × n 个参数以及 O(k × n) 的 运行时间。在很多应用方面，只需保持 k 的数量级远小于 m，就能在机器学习的任务中取得好的表现。
参数共享 (parameter sharing) 是指在一个模型的多个函数中使用相同的参数。 在传统的神经网络中，当计算一层的输出时，权值矩阵的每一个元素只使用一次，当 它乘以输入的一个元素后就再也不会用到了。作为参数共享的同义词，我们可以说 一个网络含有绑定的权值 (tied weights)，因为用于一个输入的权值也会被绑定在其 他的权值上。在卷积神经网络中，核的每一个元素都作用在输入的每一位置上(除 了一些可能的边界像素，取决于对于边界的决策设计)。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。这虽然没有改变前向传播的时间(仍然是 O(k × n))，但它显著地把模型的存储需求降低至 k 个参数，并且 k 通常是远小于 m 的数量级。因为 m 和 n 通常规模 很接近，k 在实际中相对于 m × n 是很小的。因此，卷积在存储需求和统计效率方 面极大地优于稠密矩阵的乘法运算。

对于卷积，参数共享的特殊形式使得神经网络层具有对平移等变 (equivariance) 的性质。如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说 它是等变 (equivariant) 的。特别地，如果函数 f(x) 与 g(x) 满足 f(g(x)) = g(f(x))， 我们就说 f(x) 对于变换 g 具有等变性。对于卷积来说，如果令 g 是输入的任意平 移函数，那么卷积函数对于 g 具有等变性。举个例子，令 I 表示图像的明亮度函数
(取值为整数)，g 表示图像函数的变换函数(把一个图像函数映射到另一个图像函 数的函数)使得 I′ = g(I)，其中 I′(x, y) = I(x − 1, y)。这个函数把 I 中的每个像素 向右移动一格。如果我们先对 I 进行这种变换然后进行卷积操作所得到的结果，与 先对 I 进行卷积然后再对输出使用平移函数 g 得到的结果是一样的。当处理时间序列数据时，卷积产生一条用来表明输入中出现不同特征的某种时间轴。如果我们把输入中的一个事件向后延时，在输出中也会有完全相同的表示，只是时间延时了。 图像与之类似，卷积产生了一个 2 维映射来表明某种属性在输入的什么位置出现了。 如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。
